<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on David Wilde</title>
        <link>https://david.wilde-ventures.com/posts/</link>
        <description>Recent content in Posts on David Wilde</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
        <lastBuildDate>Sun, 30 Jul 2023 16:53:13 +0200</lastBuildDate>
        <atom:link href="https://david.wilde-ventures.com/posts/index.xml" rel="self" type="application/rss+xml" />
        
        <item>
            <title>On AI and Fears About the Future</title>
            <link>https://david.wilde-ventures.com/posts/on-ai-and-fears-about-the-future/</link>
            <pubDate>Sun, 30 Jul 2023 16:53:13 +0200</pubDate>
            
            <guid>https://david.wilde-ventures.com/posts/on-ai-and-fears-about-the-future/</guid>
            <description>Preamble My thoughts are inspired by listening to Lex Fridman’s podcast episode with Yuval Noah Harari1. I really enjoyed listening to it, it was inspiring to me, thought provoking. Some arguments he made in the interview I don&amp;rsquo;t fully subscribe to. I try to write down my thoughts about them here.
Roughly paraphrased I understood Harari’s point on future risks of AI2 as follows:
AI is a new type of tool: it can make decisions on its own and generate ideas on its own AI will likely understand us (humans) very well, but we don’t understand it AI and bio-engineering might be used to induce change in humans to serve the nefarious ends of powerful, authoritarian regimes To point 3 - Technology Misuse I agree.</description>
            <content type="html"><![CDATA[<h2 id="preamble">Preamble</h2>
<p>My thoughts are inspired by listening to Lex Fridman’s podcast episode with Yuval Noah Harari<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.
I really enjoyed listening to it, it was inspiring to me, thought provoking.
Some arguments he made in the interview I don&rsquo;t fully subscribe to. I try to write down my thoughts about them here.</p>
<p>Roughly paraphrased I understood Harari’s point on future risks of AI<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  as follows:</p>
<ol>
<li>AI is a new type of tool: it can make decisions on its own and generate ideas on its own</li>
<li>AI will likely understand us (humans) very well, but we don’t understand it</li>
<li>AI and bio-engineering might be used to induce change in humans to serve the nefarious ends of powerful, authoritarian regimes</li>
</ol>
<h2 id="to-point-3---technology-misuse">To point 3 - Technology Misuse</h2>
<p>I agree. It sounds plausible to me that once the technology is available to gain an advantage, i.e. move closer to ones goals, it will be used in that way.
Humans use them for “good” and for “evil”. As an example: 1) AlphaFold from DeepMind is speeding up biology research to a new degree.
2) With the availability of crypto currencies and their ease of sending money via uncontrolled channels, cyber criminals had a hay day (see the influx of ransom ware attacks in the past 5 years).
Similarly, we might observe how the generative text capabilities of large language models will be used to generate scam and phishing campaigns that will appear more authentic.</p>
<p>I cannot and don’t want to forecast a trend how this might evolve with more capable technology.
All I can see from my perspective, with my work experience in detecting malicious behavior in the digital domain, is that there always has been a cat and mouse game.
Sometimes the mouse scored a lot of cheese before the cat could figure out the tactics to catch it, but the cat never starved.
Therefore, while I see Harari’s concerns I am vaguely optimistic that we will figure out ways to cope with it.</p>
<h2 id="to-point-2---understanding">To point 2 - Understanding</h2>
<p>I am not sure what “breed” of AI Harari imagined in this interview. From my understanding no model is able to understand humans to the extent to manipulate them with intent.
It is true that we don’t know in detail how large language models gain their capability and generate their output after being trained on large quantities of text.</p>
<p>Where manipulation takes place, I’d argue, is in recommendation engines. Prominent examples are the systems that fill the feeds of Facebook, Twitter and YouTube.
On a high level, here the systems are fed with user interaction data and set to predict certain desired outcomes, coupled with an optimization function.
The manipulations comes from the desired target metric during the training of these systems. Not all aspects of that manipulation (one could also call it nudging or recommending in most cases) are intentional.
This is because it is difficult to think the implication and effects of these systems through to the nth-degree.
The reason for that lies in the complexity of the problem.
The creators of these system have a goal in their mind and have to assume that the data they collect and the target they set will achieve that goal.
The relationship between the training data, the target and the goal is not linear, not trivial, because it involves human behavior—in groups,
not even just individual (if I learned one thing from my undergrad in economics its that we are really bad at predicting group interactions).
One cannot think through this problem and come up with an equation to solve it, not at least with a proper serving of uncertainty.</p>
<p>That is why ML algorithms, equations that adjust parameters based on training data and an objective, are useful.
With ML as a tool, there is no need to develop a theory and deduce a model from it. Instead it is a practical exercise to train the system, see where it fails and adapt from there.
This exploratory approach brings along the fact that the map of its unintended side effects is incomplete.</p>
<p>ML systems do the thing they were trained to do.
And as with all types of computer behavior, the computer follows what the human told it to do to the word, even if the human meant it another way.
These large language models don’t understand us the same way a good therapist would.
They detect patterns and act according to their training.</p>
<h2 id="to-point-1---decisions-and-ideas">To point 1 - Decisions and Ideas</h2>
<p>Again, I am not sure what Harari meant here.</p>
<h3 id="decision-systems">Decision Systems</h3>
<p>Rule-based decision system were making decisions by themselves even before a wide adoption of ML systems in the domains he mentioned, credit and loan scoring.
ML systems are in most cases better at that than hand-written rules.</p>
<p>I will now assume for the sake of argument that Harari meant that “AI” systems would follow their own agenda and base their decisions on that.
In that case I don’t know of evidence today that “AI” systems are capable of that.
The agenda, i.e. objective or target, of those systems today is set by humans.
As described in “to point two”: ML systems fall to their level of training.</p>
<h3 id="idea-generation-is-different">Idea generation is different</h3>
<p>A decision system backed by rules or a trained ML model for the application of credit scoring does not generate ideas.
Even with large language models I’d say they are rehashing what they’ve learned and are not generating truly new ideas.
But then comes the whole debate if humans are any different.</p>
<p>An example in the other direction could be AlphaFold.
When it discovers new protein structures can one describe that as a new idea? Papers where written by humans before on such discoveries.
I have a hard time to draw the analogy to systems like GPT-4.
AlphaFold was specifically designed to do that and is in essence exploring the space of possible protein structure.
We can generate new discoveries more efficiently than with the tools before.
However, are all idea discoveries exploration of some space? I don’t know.
I feel this question is out of my area of expertise; might need to ask a psychologist and philosopher about that.</p>
<p>Nevertheless, I’m skeptic and hesitant to describe LLMs as they are today as idea generators.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p><a href="https://lexfridman.com/yuval-noah-harari/">Yuval Noah Harari: Human Nature, Intelligence, Power, and Conspiracies | Lex Fridman Podcast #390</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>See from timestamp 2:06:59, &ldquo;AI safety&rdquo; in the episode&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content>
        </item>
        
        <item>
            <title>My Issue With LLM in Programming</title>
            <link>https://david.wilde-ventures.com/posts/my-issue-with-llm-in-programming/</link>
            <pubDate>Wed, 22 Mar 2023 09:31:06 +0100</pubDate>
            
            <guid>https://david.wilde-ventures.com/posts/my-issue-with-llm-in-programming/</guid>
            <description>There are organizations (example, nothing in particular about this Org, I stumbled over their blog post recently, which inspired the thoughts below) trying to build LLM-based systems that can program. In the example posted above they describe the difficulties with LLM and accessing real-word applications like databases.
The main issue that exists is hallucination. LLMs can write SQL, but they are often prone to making up tables, making up fields, and generally just writing SQL that if executed against your database would not actually be valid.</description>
            <content type="html"><![CDATA[<p>There are organizations (<a href="https://blog.langchain.dev/llms-and-sql/">example</a>,
nothing in particular about this Org, I stumbled over their blog post recently,
which inspired the thoughts below) trying to build LLM-based systems that can
program. In the example posted above they describe the difficulties with LLM
and accessing real-word applications like databases.</p>
<blockquote>
<p>The main issue that exists is hallucination. LLMs can write SQL, but they are
often prone to making up tables, making up fields, and generally just writing
SQL that if executed against your database would not actually be valid. So
one of the big challenges we face is how to ground the LLM in reality so that
it produces valid SQL. The main idea to fix this (we will go into more detail
below) is to provide the LLM with knowledge about what actually exists in the
database and tell it to write a SQL query consistent with that. However, this
runs into a second issue - the context window length. LLMs have some context
window which limits the amount of text they can operate over. This is
relevant because SQL databases often contain a lot of information. So if we
were to naively pass in all the data to ground the LLM in reality, we would
likely run into this issue. A third issue is a more basic one: sometimes the
LLM just messes up. The SQL it writes may be incorrect for whatever reason,
or it could be correct but just return an unexpected result. What do we do
then? Do we give up?</p>
</blockquote>
<p><a href="https://blog.langchain.dev/llms-and-sql/">Source</a></p>
<p>Without being able to prove it, I think that we cannot simply let LLMs write
the entire script. Programming languages (counting SQL as one in this case) are
structured, filled with rules and require precision. LLMs cannot provide that
100% of the time.</p>
<p>There is the argument that humans can’t do it either, and to this I’d respond
that first, a human can self-check if the code they are writing is working and
second, someone else needs to prove-read and try out that code. Or, we want to
live dangerously and push straight to production without tests and reviews.</p>
<p>The next argument pro-LLM use is that it should augment the human. Yes, I
agree, but nothing I’ve seen so far comes close to a good integration. What, in
my opinion, would be helpful is to have a LLM system be at the the stage in the
UX as linters and static code checkers. This would require LLMs to be much more
dependable and not like a junior dev. Example: Pylint is great at telling me
where and how my Python code violates its rules. Sometimes it even suggests a
fix. Pylint was built by humans, agreeing on these rules. Now imaging a
LLM-based refactoring assistant that could suggest improvements towards
clean-code and higher performance and is tied to certain rules. In this case I
want to avoid halucinations. As a developer I don’t need another source of
uncertain suggestions, my brain is likely providing enough of those.</p>
<p>The longer I think about it, the more I think the hopes, visions and goals we
base on the current capabilities of LLMs don’t stand on robust ground. LLMs
have capabilities and we don’t understand their extent. We don’t know their
failure modes and where we can trust them. For all that more research is
required. This can take the form of companies integrating them and failing and
of academics methodically figuring them out. But as a business that has no
expertise and willingness to dive deep into LLMs, I would not go that route and
rather build/use proven, boring software.</p>
]]></content>
        </item>
        
        <item>
            <title>Writing Julia Code Examples for Taleb&#39;s &#39;Statistical Consequences of Fat Tails&#39;</title>
            <link>https://david.wilde-ventures.com/posts/writing-julia-code-examples-scft/</link>
            <pubDate>Tue, 10 Aug 2021 20:15:20 +0200</pubDate>
            
            <guid>https://david.wilde-ventures.com/posts/writing-julia-code-examples-scft/</guid>
            <description>Inspired Tinkering I attended the Real World Risk Institute Seminar (iteration 15) earlier in 2021 and simultaneously started visiting the global reading club meetup of Nassim Nicholas Taleb&amp;rsquo;s first book of the technical Incerto series: &amp;ldquo;Statistical Consequences of Fat Tails&amp;rdquo;.
Among other things both events inspired me to &amp;ldquo;code-up&amp;rdquo; statistical examples, using the Julia language.
The first example can be seen here, and code can be checked-out here.
My current plan&amp;ndash;undoubtedly changing in the future&amp;ndash;is to add these examples to this repository</description>
            <content type="html"><![CDATA[<h2 id="inspired-tinkering">Inspired Tinkering</h2>
<p>I attended the <a href="https://realworldrisk.com"><em>Real World Risk Institute</em> Seminar</a> (iteration 15) earlier in 2021 and simultaneously started visiting the <a href="https://www.meetup.com/de-DE/global-technical-incerto-reading-club/">global reading club meetup</a> of Nassim Nicholas Taleb&rsquo;s first book of the technical Incerto series: <a href="https://arxiv.org/abs/2001.10488">&ldquo;Statistical Consequences of Fat Tails&rdquo;</a>.</p>
<p>Among other things both events inspired me to &ldquo;code-up&rdquo; statistical examples, using the <a href="https://arxiv.org/abs/2001.10488">Julia language</a>.</p>
<p>The first example can be seen <a href="https://david.wilde-ventures.com/SCFT/chapter04_01.html">here</a>,
and code can be checked-out <a href="https://github.com/davidchris/RWRI15Tinkering.jl/blob/main/src/SCFT-examples/chapter_04.jl">here</a>.</p>
<p>My current plan&ndash;undoubtedly changing in the future&ndash;is to add these examples to this <a href="https://github.com/davidchris/RWRI15Tinkering.jl">repository</a></p>
<h2 id="why">Why?</h2>
<p>As to the reason why I&rsquo;m doing this: practice&ndash;practice in the Julia language and practice of mathematical and statistical understanding&ndash;then, I will see what to make of this next.</p>
]]></content>
        </item>
        
        <item>
            <title>Creating This Website</title>
            <link>https://david.wilde-ventures.com/posts/creating-this-website/</link>
            <pubDate>Mon, 25 May 2020 09:10:55 +0000</pubDate>
            
            <guid>https://david.wilde-ventures.com/posts/creating-this-website/</guid>
            <description>This is the start of my personal website. And instead of writing some generic, somewhat empty first post saying &amp;lsquo;Hello World!&amp;rsquo;. I instead want to write about how I created this website. Foremost as a tutorial for myself.
Static Site Generation I went with Hugo, after some research online:
You could all build it by yourself Jekyll, generates html with themes from markdown, runs locally, well integrated with GitHub, written in Ruby (I think) Hugo, fast, works everywhere, easy to use, written in Go And yes, it&amp;rsquo;s fast and easy to use.</description>
            <content type="html"><![CDATA[<p>This is the start of my personal website. And instead of writing some generic, somewhat empty first post saying &lsquo;Hello World!&rsquo;. I instead want to write about how I created this website. Foremost as a tutorial for myself.</p>
<h2 id="static-site-generation">Static Site Generation</h2>
<p>I went with Hugo, after some research online:</p>
<ul>
<li>You could all build it by yourself</li>
<li>Jekyll, generates html with themes from markdown, runs locally, well integrated with GitHub, written in Ruby (I think)</li>
<li>Hugo, fast, works everywhere, easy to use, written in Go</li>
</ul>
<p>And yes, it&rsquo;s fast and easy to use. I rebuild my website from scratch several times one day and there was no hustle. Utilizing one of the many themes available (<a href="https://themes.gohugo.io">Hugo Themes</a>) is also straight forward.</p>
<p>For this site I used the <a href="https://themes.gohugo.io/themes/hugo-theme-hello-friend-ng/">hello-friend-ng theme</a> from <a href="https://atlialp.com">Djordje Atlialp</a>.</p>
<h2 id="domain">Domain</h2>
<p>I probably didn&rsquo;t do it properly here, let&rsquo;s see:</p>
<p>I bought or rented a domain, actually for another project, with GoDaddy. It seemed straight forward for me. Then, for some reason, I decided to create a dynamic domain name server (DDNS) not with GoDaddy, but with dnyu.com.</p>
<p>Though I forgot why I did that, it turns out that this was the easier choice for me. Managing my DNS from dnyu is more convenient. The UI looks older, but reacts faster and everything is more easily accessible.</p>
<h2 id="hosting">Hosting</h2>
<p>It was a rough ride here.</p>
<p>First I could not decide for several days whether to host it at home, on a VM in the cloud or via GitHub/Gitlab. I figured that I want it somewhat in my own hands while not doing everything by myself.</p>
<p>For now I went with hosting over GitHub pages. After testing GitLab pages and not wanting to go through setting up a VM myself for now it seemed like the best choice.</p>
<p><strong>Edit 2021-08-09</strong>: Today I set the page to enforce https&ndash;even though it is just a static page, some browser (will) expect https to be there.
This broke my page: theme was not working, and links were broken (i.e. adding the url twice behind each other).
A fix I found here: <a href="https://github.com/matcornic/hugo-theme-learn/issues/204">Theme on github pages not working</a>.</p>
<ul>
<li>I changed the <code>baseURL</code> to include the https</li>
<li>remove the contents of the <code>public</code> folder</li>
<li>rebuild the site</li>
</ul>
<p>Now it seems to be fixed.</p>
<h2 id="publishing">Publishing</h2>
<p>Publishing goes as easy as described here: <a href="https://gohugo.io/hosting-and-deployment/hosting-on-github/">Deploy Hugo as a GitHub Pages project</a>.</p>
<p><strong>Edit 2021-08-09</strong>: Something I forgot to add here which confused me a lot in the last few days is to have my <code>public/</code> directory as a git sub-module.
This sub-module has the GitHub Page as its remote.
The &lsquo;rest&rsquo; of the code, the repository, is the source to generate the page, which goes into another git repository.</p>
<p>Thanks to: <a href="https://www.mytechramblings.com/posts/create-a-website-with-hugo-and-gh/">Create and host a blog with Hugo and GitHub Pages in less than 30 minutes</a></p>
]]></content>
        </item>
        
    </channel>
</rss>
